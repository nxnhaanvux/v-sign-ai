import React, { useRef, useState, useEffect, useCallback } from 'react';
import Webcam from 'react-webcam';
import * as HandsModule from '@mediapipe/hands';
import * as CameraModule from '@mediapipe/camera_utils';
import * as DrawingModule from '@mediapipe/drawing_utils';
import * as tf from '@tensorflow/tfjs';

// Import components
import LoadingScreen from './components/LoadingScreen';
import Header from './components/Header';
import CameraSection from './components/CameraSection';
import PredictionSection from './components/PredictionSection';
import InfoPanel from './components/InfoPanel';

function App() {
  // ===== REFS =====
  const webcamRef = useRef(null);
  const canvasRef = useRef(null);
  const handsRef = useRef(null);
  const cameraRef = useRef(null);
  const modelRef = useRef(null);
  const labelsRef = useRef({});
  
  // ===== STATE =====
  const [model, setModel] = useState(null);
  const [labels, setLabels] = useState({});
  const [prediction, setPrediction] = useState('');
  const [confidence, setConfidence] = useState(0);
  const [isLoading, setIsLoading] = useState(true);
  const [loadingProgress, setLoadingProgress] = useState(0);
  const [sequenceBuffer, setSequenceBuffer] = useState([]);
  const [fps, setFps] = useState(0);
  const [isHandDetected, setIsHandDetected] = useState(false);
  const [error, setError] = useState(null);
  
  // ===== CONSTANTS =====
  const SEQUENCE_LENGTH = 30;
  const CONFIDENCE_THRESHOLD = 0.7;
  const MODEL_PATH = '/tfjs_model/model.json';
  const LABELS_PATH = '/tfjs_model/labels.json';
  
  // ===== LOAD MODEL & LABELS =====
  useEffect(() => {
    const loadResources = async () => {
      try {
        setLoadingProgress(10);
        
        // Load TensorFlow.js model
        console.log('Loading TensorFlow.js model...');
        const loadedModel = await tf.loadLayersModel(MODEL_PATH);
        setModel(loadedModel);
        modelRef.current = loadedModel;
        console.log('‚úì Model loaded successfully');
        setLoadingProgress(50);
        
        // Warm up the model
        console.log('Warming up model...');
        const dummyInput = tf.zeros([1, 30, 126]); // 126 thay v√¨ 63
        loadedModel.predict(dummyInput).dispose();
        dummyInput.dispose();
        console.log('‚úì Model warmed up');
        setLoadingProgress(70);
        
        // Load labels
        console.log('Loading labels...');
        const response = await fetch(LABELS_PATH);
        if (!response.ok) {
          throw new Error('Failed to load labels');
        }
        const labelData = await response.json();
        setLabels(labelData);
        labelsRef.current = labelData;
        console.log('‚úì Labels loaded:', labelData);
        setLoadingProgress(90);
        
        // Initialize MediaPipe Hands
        console.log('Initializing MediaPipe Hands...');
        await initializeMediaPipe();
        console.log('‚úì MediaPipe initialized');
        setLoadingProgress(100);
        
        setTimeout(() => setIsLoading(false), 500);
        
      } catch (error) {
        console.error('Error loading resources:', error);
        setError(`Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh AI: ${error.message}`);
        setIsLoading(false);
      }
    };
    
    loadResources();
    
    // Cleanup
    return () => {
      if (cameraRef.current) {
        cameraRef.current.stop();
      }
      if (handsRef.current) {
        handsRef.current.close();
      }
    };
  }, []);
  
  // ===== INITIALIZE MEDIAPIPE =====
  const initializeMediaPipe = async () => {
        // Ki·ªÉm tra constructor an to√†n cho Production
    const HandsConstructor = HandsModule.Hands || window.Hands;

    const hands = new HandsConstructor({
      locateFile: (file) => {
        const version = HandsModule.VERSION || '0.4.1646424915'; 
        return `https://cdn.jsdelivr.net/npm/@mediapipe/hands@${version}/${file}`;
      }
    });

    hands.setOptions({
      maxNumHands: 2,           // S·ª≠a l·ªói ch√≠nh t·∫£ v√† b·∫≠t 2 tay
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.7
    });
    
    hands.onResults(onHandsResults);
    handsRef.current = hands;
  };
  
  // ===== START CAMERA =====
  useEffect(() => {
    if (!isLoading && webcamRef.current?.video && handsRef.current) {
      console.log('Starting camera...');
      
      const CameraConstructor = CameraModule.Camera || window.Camera;
      const camera = new CameraConstructor(webcamRef.current.video, {
        onFrame: async () => {
          if (webcamRef.current?.video && handsRef.current) {
            await handsRef.current.send({ image: webcamRef.current.video });
          }
        },
        width: 640,
        height: 480
      });
      
      camera.start();
      cameraRef.current = camera;
      
      // FPS counter
      let frameCount = 0;
      let lastTime = Date.now();
      
      const fpsInterval = setInterval(() => {
        const now = Date.now();
        const deltaTime = (now - lastTime) / 1000;
        const currentFps = Math.round(frameCount / deltaTime);
        setFps(currentFps);
        frameCount = 0;
        lastTime = now;
      }, 1000);
      
      const incrementFrame = () => {
        frameCount++;
        requestAnimationFrame(incrementFrame);
      };
      incrementFrame();
      
      return () => {
        clearInterval(fpsInterval);
        if (cameraRef.current) {
          cameraRef.current.stop();
        }
      };
    }
  }, [isLoading]);
  
  // ===== PROCESS HAND LANDMARKS =====
  const onHandsResults = useCallback((results) => {
      drawLandmarksOnCanvas(results);
      
      const handDetected = results.multiHandLandmarks && results.multiHandLandmarks.length > 0;
      setIsHandDetected(handDetected);
      
      if (handDetected) {
          let flattenedLandmarks;
          
          // X·ª≠ l√Ω 1 ho·∫∑c 2 tay th√†nh vector 126 chi·ªÅu
          if (results.multiHandLandmarks.length === 1) {
              const hand1 = results.multiHandLandmarks[0].flatMap(lm => [lm.x, lm.y, lm.z]);
              const hand2Padding = new Array(63).fill(0);
              flattenedLandmarks = [...hand1, ...hand2Padding];
          } else {
              const hand1 = results.multiHandLandmarks[0].flatMap(lm => [lm.x, lm.y, lm.z]);
              const hand2 = results.multiHandLandmarks[1].flatMap(lm => [lm.x, lm.y, lm.z]);
              flattenedLandmarks = [...hand1, ...hand2];
          }
          
          setSequenceBuffer(prev => {
              const newBuffer = [...prev, flattenedLandmarks];
              
              // In ra ƒë·ªÉ xem s·ªë frame ƒëang ch·∫°y (R·∫•t h·ªØu √≠ch ƒë·ªÉ debug)
              console.log(`ƒêang gom frame: ${newBuffer.length}/30`);
              
              if (newBuffer.length > SEQUENCE_LENGTH) {
                  newBuffer.shift();
              }
              
              if (newBuffer.length === SEQUENCE_LENGTH && modelRef.current) {
                  console.log("üü¢ ƒê√£ gom ƒë·ªß 30 frames! B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n...");
                  predictGesture(newBuffer);
              }
              
              return newBuffer;
          });
      } else {
    // KHI M·∫§T D·∫§U TAY: 
    // Ch√∫ng ta KH√îNG x√≥a m·∫£ng sequenceBuffer n·ªØa, c·ª© gi·ªØ ƒë√≥ ch·ªù tay xu·∫•t hi·ªán l·∫°i.
    // Ch·ªâ c·∫ßn ·∫©n ƒëi k·∫øt qu·∫£ ch·ªØ tr√™n m√†n h√¨nh th√¥i.
    setPrediction('');
    setConfidence(0);
}
  }, [model]);
  
  // ===== DRAW LANDMARKS ON CANVAS =====
  const drawLandmarksOnCanvas = (results) => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext('2d');
    
    ctx.save();
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    if (results.multiHandLandmarks) {
      // Th·ª≠ l·∫•y h√†m v·∫Ω t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªÉ tr√°nh l·ªói Production
      const drawConnect = DrawingModule.drawConnectors || window.drawConnectors;
      const drawLand = DrawingModule.drawLandmarks || window.drawLandmarks;
      const connections = HandsModule.HAND_CONNECTIONS || window.HAND_CONNECTIONS;

      if (drawConnect && drawLand) {
        results.multiHandLandmarks.forEach((landmarks) => {
          // V·∫Ω d√¢y xanh (Connectors)
          drawConnect(ctx, landmarks, connections, {
            color: '#00FF00',
            lineWidth: 4
          });

          // V·∫Ω ƒëi·ªÉm ƒë·ªè (Landmarks)
          drawLand(ctx, landmarks, {
            color: '#FF0000',
            lineWidth: 2,
            radius: 5
          });
        });
      } else {
        console.error("DrawingUtils not loaded properly");
      }
    }
    ctx.restore();
  };
  
  const predictGesture = async (sequence) => {
    if (!modelRef.current) return;
    
    try {
      const inputTensor = tf.tensor3d([sequence], [1, 30, 126]);
      
      const predictions = modelRef.current.predict(inputTensor);
      const predictionsArray = await predictions.data();
      
      const maxIndex = predictionsArray.indexOf(Math.max(...predictionsArray));
      const maxConfidence = predictionsArray[maxIndex];
      const gestureName = labelsRef.current[maxIndex];
      
      console.log(`ü§ñ ƒêo√°n: ${gestureName} - T·ª± tin: ${(maxConfidence * 100).toFixed(2)}%`);
      
      if (maxConfidence >= CONFIDENCE_THRESHOLD) { 
        console.log("‚úÖ ƒê·ªß t·ª± tin! B·∫ÆT ƒê·∫¶U IN CH·ªÆ L√äN M√ÄN H√åNH N√ÄY:", gestureName);
        setPrediction(gestureName); // L·ªánh n√†y s·∫Ω y√™u c·∫ßu giao di·ªán hi·ªán ch·ªØ
        setConfidence(maxConfidence);
      } else {
        console.log("‚ö†Ô∏è T·ª± tin qu√° th·∫•p, kh√¥ng in!");
      }
      
      // CH·ªà HI·ªÇN TH·ªä KHI ƒê·ª¶ T·ª∞ TIN
      if (maxConfidence >= CONFIDENCE_THRESHOLD) { 
        setPrediction(gestureName || '');
        setConfidence(maxConfidence);
      } 
      // X√ìA KH·ªêI ELSE ƒêI! 
      // Kh√¥ng g·ªçi setPrediction('') ·ªü ƒë√¢y n·ªØa ƒë·ªÉ gi·ªØ l·∫°i ch·ªØ tr√™n m√†n h√¨nh cho m·∫Øt k·ªãp ƒë·ªçc.
      else {
        console.log("‚ö†Ô∏è T·ª± tin th·∫•p -> B·ªè qua, gi·ªØ nguy√™n ch·ªØ c≈© tr√™n m√†n h√¨nh");
      }
      
      inputTensor.dispose();
      predictions.dispose();
      
    } catch (error) {
      console.error('L·ªói khi d·ª± ƒëo√°n:', error);
    }
  };
  
  // ===== RENDER =====
  if (error) {
    return (
      <div className="min-h-screen bg-gradient-to-br from-red-50 to-red-100 flex items-center justify-center p-8">
        <div className="bg-white rounded-2xl shadow-2xl p-8 max-w-md">
          <div className="text-red-600 text-6xl mb-4 text-center">‚ö†Ô∏è</div>
          <h2 className="text-2xl font-bold text-red-900 mb-4 text-center">
            L·ªói T·∫£i M√¥ h√¨nh
          </h2>
          <p className="text-gray-700 mb-6 text-center">{error}</p>
          <div className="bg-red-50 border border-red-200 rounded-lg p-4 mb-6">
            <p className="text-sm text-red-800">
              <strong>H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c:</strong>
            </p>
            <ul className="text-sm text-red-700 mt-2 ml-4 list-disc">
              <li>ƒê·∫£m b·∫£o th∆∞ m·ª•c <code className="bg-red-100 px-1 rounded">public/tfjs_model/</code> t·ªìn t·∫°i</li>
              <li>Ki·ªÉm tra file <code className="bg-red-100 px-1 rounded">model.json</code> v√† <code className="bg-red-100 px-1 rounded">labels.json</code></li>
              <li>Ch·∫°y script <code className="bg-red-100 px-1 rounded">convert_to_tfjs.py</code> ƒë·ªÉ t·∫°o m√¥ h√¨nh</li>
            </ul>
          </div>
          <button
            onClick={() => window.location.reload()}
            className="w-full bg-red-600 hover:bg-red-700 text-white font-semibold py-3 px-6 rounded-lg transition-colors"
          >
            T·∫£i l·∫°i Trang
          </button>
        </div>
      </div>
    );
  }
  
  if (isLoading) {
    return <LoadingScreen progress={loadingProgress} />;
  }
  
  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 via-indigo-50 to-purple-50">
      {/* Header */}
      <Header fps={fps} bufferLength={sequenceBuffer.length} maxBuffer={SEQUENCE_LENGTH} />
      
      {/* Main Content */}
      <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
        <div className="grid grid-cols-1 lg:grid-cols-3 gap-8">
          {/* Camera Section - 2 columns */}
          <div className="lg:col-span-2">
            <CameraSection
              webcamRef={webcamRef}
              canvasRef={canvasRef}
              isHandDetected={isHandDetected}
              sequenceBuffer={sequenceBuffer}
              maxBuffer={SEQUENCE_LENGTH}
            />
          </div>
          
          {/* Prediction Section - 1 column */}
          <div className="lg:col-span-1">
            <PredictionSection
              prediction={prediction}
              confidence={confidence}
              labels={labels}
              sequenceBuffer={sequenceBuffer}
              isHandDetected={isHandDetected}
            />
          </div>
        </div>
        
        {/* Info Panel */}
        <InfoPanel />
      </div>
      
      {/* Footer */}
      <footer className="bg-white bg-opacity-80 backdrop-blur-sm border-t border-gray-200 mt-12">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6">
          <div className="text-center text-gray-600">
            <p className="mb-2">
              <strong className="text-indigo-600">V-Sign AI</strong> - Ph√° b·ªè r√†o c·∫£n giao ti·∫øp cho 2.5 tri·ªáu ng∆∞·ªùi khi·∫øm th√≠nh t·∫°i Vi·ªát Nam
            </p>
            <p className="text-sm">
              C√¥ng ngh·ªá: React + MediaPipe Hands + TensorFlow.js | 
              <span className="text-green-600 font-semibold"> 100% Client-side Processing</span>
            </p>
            <p className="text-xs text-gray-500 mt-2">
              ¬© 2024 V-Sign AI Team. M√£ ngu·ªìn m·ªü - MIT License
            </p>
          </div>
        </div>
      </footer>
    </div>
  );
}

export default App;